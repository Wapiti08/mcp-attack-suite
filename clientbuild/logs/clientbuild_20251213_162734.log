2025-12-13 16:27:36.034 INFO mcp_hub: mcp_hub.connect servers=['mcp1']
2025-12-13 16:27:36.037 DEBUG mcp.client.streamable_http: Connecting to StreamableHTTP endpoint: http://0.0.0.0:8000/mcp
2025-12-13 16:27:36.199 DEBUG mcp.client.streamable_http: Sending client message: root=JSONRPCRequest(method='initialize', params={'protocolVersion': '2025-06-18', 'capabilities': {}, 'clientInfo': {'name': 'mcp', 'version': '0.1.0'}}, jsonrpc='2.0', id=0)
2025-12-13 16:27:36.201 DEBUG httpcore.connection: connect_tcp.started host='0.0.0.0' port=8000 local_address=None timeout=30 socket_options=None
2025-12-13 16:27:36.206 DEBUG httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1076b0710>
2025-12-13 16:27:36.207 DEBUG httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-12-13 16:27:36.209 DEBUG httpcore.http11: send_request_headers.complete
2025-12-13 16:27:36.209 DEBUG httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-12-13 16:27:36.210 DEBUG httpcore.http11: send_request_body.complete
2025-12-13 16:27:36.210 DEBUG httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-12-13 16:27:36.232 DEBUG httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 13 Dec 2025 15:27:35 GMT'), (b'server', b'uvicorn'), (b'cache-control', b'no-cache, no-transform'), (b'connection', b'keep-alive'), (b'content-type', b'text/event-stream'), (b'mcp-session-id', b'261d0aacc1814cb38056884a2d8a0d37'), (b'x-accel-buffering', b'no'), (b'transfer-encoding', b'chunked')])
2025-12-13 16:27:36.233 INFO httpx: HTTP Request: POST http://0.0.0.0:8000/mcp "HTTP/1.1 200 OK"
2025-12-13 16:27:36.233 INFO mcp.client.streamable_http: Received session ID: 261d0aacc1814cb38056884a2d8a0d37
2025-12-13 16:27:36.233 DEBUG httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-12-13 16:27:36.234 DEBUG mcp.client.streamable_http: SSE message: root=JSONRPCResponse(jsonrpc='2.0', id=0, result={'protocolVersion': '2025-06-18', 'capabilities': {'experimental': {}, 'prompts': {'listChanged': True}, 'resources': {'subscribe': False, 'listChanged': True}, 'tools': {'listChanged': True}}, 'serverInfo': {'name': 'WeatherCurrentServer', 'version': '1.3.0'}, 'instructions': 'Provides current weather conditions globally'})
2025-12-13 16:27:36.235 INFO mcp.client.streamable_http: Negotiated protocol version: 2025-06-18
2025-12-13 16:27:36.235 DEBUG httpcore.http11: response_closed.started
2025-12-13 16:27:36.235 DEBUG httpcore.http11: response_closed.complete
2025-12-13 16:27:36.236 DEBUG mcp.client.streamable_http: Sending client message: root=JSONRPCNotification(method='notifications/initialized', params=None, jsonrpc='2.0')
2025-12-13 16:27:36.237 INFO mcp_hub: mcp_hub.refresh_tools start
2025-12-13 16:27:36.238 DEBUG httpcore.connection: connect_tcp.started host='0.0.0.0' port=8000 local_address=None timeout=30 socket_options=None
2025-12-13 16:27:36.239 DEBUG httpcore.connection: connect_tcp.started host='0.0.0.0' port=8000 local_address=None timeout=30 socket_options=None
2025-12-13 16:27:36.241 DEBUG httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1076b1c10>
2025-12-13 16:27:36.241 DEBUG httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-12-13 16:27:36.241 DEBUG httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1076b1bd0>
2025-12-13 16:27:36.241 DEBUG httpcore.http11: send_request_headers.complete
2025-12-13 16:27:36.242 DEBUG httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-12-13 16:27:36.242 DEBUG httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-12-13 16:27:36.242 DEBUG httpcore.http11: send_request_body.complete
2025-12-13 16:27:36.242 DEBUG httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-12-13 16:27:36.243 DEBUG httpcore.http11: send_request_headers.complete
2025-12-13 16:27:36.243 DEBUG httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-12-13 16:27:36.243 DEBUG httpcore.http11: send_request_body.complete
2025-12-13 16:27:36.243 DEBUG httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-12-13 16:27:36.243 DEBUG httpcore.http11: receive_response_body.failed exception=GeneratorExit()
2025-12-13 16:27:36.244 DEBUG httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 202, b'Accepted', [(b'date', b'Sat, 13 Dec 2025 15:27:35 GMT'), (b'server', b'uvicorn'), (b'content-type', b'application/json'), (b'mcp-session-id', b'261d0aacc1814cb38056884a2d8a0d37'), (b'content-length', b'0')])
2025-12-13 16:27:36.245 INFO httpx: HTTP Request: POST http://0.0.0.0:8000/mcp "HTTP/1.1 202 Accepted"
2025-12-13 16:27:36.245 DEBUG mcp.client.streamable_http: Received 202 Accepted
2025-12-13 16:27:36.245 DEBUG httpcore.http11: response_closed.started
2025-12-13 16:27:36.246 DEBUG httpcore.http11: response_closed.complete
2025-12-13 16:27:36.246 DEBUG mcp.client.streamable_http: Sending client message: root=JSONRPCRequest(method='tools/list', params=None, jsonrpc='2.0', id=1)
2025-12-13 16:27:36.247 DEBUG httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 13 Dec 2025 15:27:35 GMT'), (b'server', b'uvicorn'), (b'cache-control', b'no-cache, no-transform'), (b'connection', b'keep-alive'), (b'content-type', b'text/event-stream'), (b'mcp-session-id', b'261d0aacc1814cb38056884a2d8a0d37'), (b'x-accel-buffering', b'no'), (b'transfer-encoding', b'chunked')])
2025-12-13 16:27:36.247 INFO httpx: HTTP Request: GET http://0.0.0.0:8000/mcp "HTTP/1.1 200 OK"
2025-12-13 16:27:36.247 DEBUG mcp.client.streamable_http: GET SSE connection established
2025-12-13 16:27:36.247 DEBUG httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-12-13 16:27:36.248 DEBUG httpcore.connection: connect_tcp.started host='0.0.0.0' port=8000 local_address=None timeout=30 socket_options=None
2025-12-13 16:27:36.250 DEBUG httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1076c7610>
2025-12-13 16:27:36.250 DEBUG httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-12-13 16:27:36.251 DEBUG httpcore.http11: send_request_headers.complete
2025-12-13 16:27:36.251 DEBUG httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-12-13 16:27:36.251 DEBUG httpcore.http11: send_request_body.complete
2025-12-13 16:27:36.251 DEBUG httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-12-13 16:27:36.256 DEBUG httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 13 Dec 2025 15:27:35 GMT'), (b'server', b'uvicorn'), (b'cache-control', b'no-cache, no-transform'), (b'connection', b'keep-alive'), (b'content-type', b'text/event-stream'), (b'mcp-session-id', b'261d0aacc1814cb38056884a2d8a0d37'), (b'x-accel-buffering', b'no'), (b'transfer-encoding', b'chunked')])
2025-12-13 16:27:36.256 INFO httpx: HTTP Request: POST http://0.0.0.0:8000/mcp "HTTP/1.1 200 OK"
2025-12-13 16:27:36.257 DEBUG httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-12-13 16:27:36.260 DEBUG mcp.client.streamable_http: SSE message: root=JSONRPCResponse(jsonrpc='2.0', id=1, result={'tools': [{'name': 'get_current_weather_tool', 'description': 'get weather of a location', 'inputSchema': {'properties': {'location': {'type': 'string'}, 'detail': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None}}, 'required': ['location'], 'type': 'object'}, 'outputSchema': {'properties': {'result': {'items': {'type': 'string'}, 'type': 'array'}}, 'required': ['result'], 'type': 'object', 'x-fastmcp-wrap-result': True}, '_meta': {'_fastmcp': {'tags': []}}}]})
2025-12-13 16:27:36.260 DEBUG httpcore.http11: response_closed.started
2025-12-13 16:27:36.261 DEBUG httpcore.http11: response_closed.complete
2025-12-13 16:27:36.262 INFO mcp_hub: mcp_hub.refresh_tools done tools=1
2025-12-13 16:27:36.300 INFO loop: loop.turn start=1 current={"role": "user", "content": "成都现在天气怎么样？", "tool_call_id": null, "tool_calls": null}
2025-12-13 16:27:36.300 INFO planner: planner.next_action input={"role": "user", "content": "成都现在天气怎么样？", "tool_call_id": null, "tool_calls": null}
2025-12-13 16:27:36.335 INFO planner: planner.next_action output=Query messages=2 tools=1
2025-12-13 16:27:36.335 INFO loop: loop.action Query turn=1 messages=2 tools=1
2025-12-13 16:27:36.335 INFO loop: loop.llm_request model=meta-llama-3.1-8b-instruct messages=2 tools=1 last={"role": "user", "content": "成都现在天气怎么样？", "tool_call_id": null, "tool_calls": null}
2025-12-13 16:27:36.335 DEBUG loop: loop.llm_request messages_payload=[{"role": "system", "content": "你是一个会使用工具的助手。必要时调用工具，并在得到工具结果后给出最终回答。"}, {"role": "user", "content": "成都现在天气怎么样？"}]
2025-12-13 16:27:36.335 DEBUG loop: loop.llm_request tools_payload=["get_current_weather_tool"]
2025-12-13 16:27:36.350 DEBUG httpcore.http11: receive_response_body.failed exception=GeneratorExit()
2025-12-13 16:27:36.400 DEBUG openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': '你是一个会使用工具的助手。必要时调用工具，并在得到工具结果后给出最终回答。'}, {'role': 'user', 'content': '成都现在天气怎么样？'}], 'model': 'meta-llama-3.1-8b-instruct', 'parallel_tool_calls': False, 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather_tool', 'description': 'get weather of a location', 'parameters': {'properties': {'location': {'type': 'string'}, 'detail': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None}}, 'required': ['location'], 'type': 'object'}}}]}}
2025-12-13 16:27:36.402 DEBUG httpcore.connection: connect_tcp.started host='localhost' port=1234 local_address=None timeout=5.0 socket_options=None
2025-12-13 16:27:36.405 DEBUG httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1076d6ad0>
2025-12-13 16:27:36.405 DEBUG httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-12-13 16:27:36.406 DEBUG httpcore.http11: send_request_headers.complete
2025-12-13 16:27:36.406 DEBUG httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-12-13 16:27:36.406 DEBUG httpcore.http11: send_request_body.complete
2025-12-13 16:27:36.406 DEBUG httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-12-13 16:27:38.715 DEBUG httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'780'), (b'ETag', b'W/"30c-/i/dledQSo+ZKC/CQvfKbtpu644"'), (b'Date', b'Sat, 13 Dec 2025 15:27:38 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-13 16:27:38.717 INFO httpx: HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-13 16:27:38.718 DEBUG httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-12-13 16:27:38.718 DEBUG httpcore.http11: receive_response_body.complete
2025-12-13 16:27:38.718 DEBUG httpcore.http11: response_closed.started
2025-12-13 16:27:38.718 DEBUG httpcore.http11: response_closed.complete
2025-12-13 16:27:38.719 DEBUG openai._base_client: HTTP Request: POST http://localhost:1234/v1/chat/completions "200 OK"
2025-12-13 16:27:38.723 INFO loop: loop.llm_response message={"role": "assistant", "content": "", "tool_calls": ["ChatCompletionMessageToolCall(id='221154185', function=Function(arguments='{\"location\":\"成都\",\"detail\":null}', name='get_current_weather_tool'), type='function')"]}
2025-12-13 16:27:38.727 DEBUG loop: loop.llm_response raw={"id": "chatcmpl-tgndvpg4qeo5qhwp2yst0q", "choices": [{"finish_reason": "tool_calls", "index": 0, "logprobs": null, "message": {"content": "", "refusal": null, "role": "assistant", "annotations": null, "audio": null, "function_call": null, "tool_calls": [{"id": "221154185", "function": {"arguments": "{\"location\":\"成都\",\"detail\":null}", "name": "get_current_weather_tool"}, "type": "function"}]}}], "created": 1765639656, "model": "meta-llama-3.1-8b-instruct", "object": "chat.completion", "service_tier": null, "system_fingerprint": "meta-llama-3.1-8b-instruct", "usage": {"completion_tokens": 25, "prompt_tokens": 384, "total_tokens": 409, "completion_tokens_details": null, "prompt_tokens_details": null}, "stats": {}}
2025-12-13 16:27:38.728 INFO loop: loop.turn start=2 current={"role": "assistant", "content": "", "tool_calls": ["ChatCompletionMessageToolCall(id='221154185', function=Function(arguments='{\"location\":\"成都\",\"detail\":null}', name='get_current_weather_tool'), type='function')"]}
2025-12-13 16:27:38.728 INFO planner: planner.next_action input={"role": "assistant", "content": "", "tool_calls": ["ChatCompletionMessageToolCall(id='221154185', function=Function(arguments='{\"location\":\"成都\",\"detail\":null}', name='get_current_weather_tool'), type='function')"]}
2025-12-13 16:27:38.728 INFO planner: planner.next_action output=ToolCall id=221154185 name=get_current_weather_tool arguments={"location":"成都","detail":null}
2025-12-13 16:27:38.728 INFO loop: loop.action ToolCall turn=2 id=221154185 name=get_current_weather_tool arguments={"location":"成都","detail":null}
2025-12-13 16:27:38.728 INFO mcp_hub: mcp_hub.call_tool exposed=get_current_weather_tool server=mcp1 tool=get_current_weather_tool args={"location": "成都", "detail": null}
2025-12-13 16:27:38.732 DEBUG mcp.client.streamable_http: Sending client message: root=JSONRPCRequest(method='tools/call', params={'name': 'get_current_weather_tool', 'arguments': {'location': '成都', 'detail': None}, '_meta': {'progressToken': 2}}, jsonrpc='2.0', id=2)
2025-12-13 16:27:38.733 DEBUG httpcore.connection: connect_tcp.started host='0.0.0.0' port=8000 local_address=None timeout=30 socket_options=None
2025-12-13 16:27:38.734 DEBUG httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x107223210>
2025-12-13 16:27:38.735 DEBUG httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-12-13 16:27:38.735 DEBUG httpcore.http11: send_request_headers.complete
2025-12-13 16:27:38.735 DEBUG httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-12-13 16:27:38.735 DEBUG httpcore.http11: send_request_body.complete
2025-12-13 16:27:38.735 DEBUG httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-12-13 16:27:38.737 DEBUG httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 13 Dec 2025 15:27:38 GMT'), (b'server', b'uvicorn'), (b'cache-control', b'no-cache, no-transform'), (b'connection', b'keep-alive'), (b'content-type', b'text/event-stream'), (b'mcp-session-id', b'261d0aacc1814cb38056884a2d8a0d37'), (b'x-accel-buffering', b'no'), (b'transfer-encoding', b'chunked')])
2025-12-13 16:27:38.737 INFO httpx: HTTP Request: POST http://0.0.0.0:8000/mcp "HTTP/1.1 200 OK"
2025-12-13 16:27:38.737 DEBUG httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-12-13 16:27:38.752 DEBUG mcp.client.streamable_http: SSE message: root=JSONRPCResponse(jsonrpc='2.0', id=2, result={'content': [{'type': 'text', 'text': '["成都位置的天气还行"]'}], 'structuredContent': {'result': ['成都位置的天气还行']}, 'isError': False})
2025-12-13 16:27:38.752 DEBUG httpcore.http11: response_closed.started
2025-12-13 16:27:38.753 DEBUG httpcore.http11: response_closed.complete
2025-12-13 16:27:38.758 INFO mcp_hub: mcp_hub.call_tool result exposed=get_current_weather_tool server=mcp1 tool=get_current_weather_tool result='"CallToolResult(content=[TextContent(type=\'text\', text=\'[\\"成都位置的天气还行\\"]\', annotations=None, meta=None)], structured_content={\'result\': [\'成都位置的天气还行\']}, meta=None, data=[\'成都位置的天气还行\'], is_error=False)"'
2025-12-13 16:27:38.758 INFO loop: loop.tool_result turn=2 id=221154185 name=get_current_weather_tool result=CallToolResult(content=[TextContent(type='text', text='["成都位置的天气还行"]', annotations=None, meta=None)], structured_content={'result': ['成都位置的天气还行']}, meta=None, data=['成都位置的天气还行'], is_error=False)
2025-12-13 16:27:38.758 INFO loop: loop.turn start=3 current={"role": "tool", "content": "CallToolResult(content=[TextContent(type='text', text='[\"成都位置的天气还行\"]', annotations=None, meta=None)], structured_content={'result': ['成都位置的天气还行']}, meta=None, data=['成都位置的天气还行'], is_error=False)", "tool_call_id": "221154185", "tool_calls": null}
2025-12-13 16:27:38.759 INFO planner: planner.next_action input={"role": "tool", "content": "CallToolResult(content=[TextContent(type='text', text='[\"成都位置的天气还行\"]', annotations=None, meta=None)], structured_content={'result': ['成都位置的天气还行']}, meta=None, data=['成都位置的天气还行'], is_error=False)", "tool_call_id": "221154185", "tool_calls": null}
2025-12-13 16:27:38.759 INFO planner: planner.next_action output=Query messages=4 tools=1
2025-12-13 16:27:38.759 INFO loop: loop.action Query turn=3 messages=4 tools=1
2025-12-13 16:27:38.759 INFO loop: loop.llm_request model=meta-llama-3.1-8b-instruct messages=4 tools=1 last={"role": "tool", "content": "CallToolResult(content=[TextContent(type='text', text='[\"成都位置的天气还行\"]', annotations=None, meta=None)], structured_content={'result': ['成都位置的天气还行']}, meta=None, data=['成都位置的天气还行'], is_error=False)", "tool_call_id": "221154185", "tool_calls": null}
2025-12-13 16:27:38.759 DEBUG loop: loop.llm_request messages_payload=[{"role": "system", "content": "你是一个会使用工具的助手。必要时调用工具，并在得到工具结果后给出最终回答。"}, {"role": "user", "content": "成都现在天气怎么样？"}, {"role": "assistant", "content": "", "tool_calls": [{"id": "221154185", "function": {"name": "get_current_weather_tool", "arguments": "{\"location\":\"成都\",\"detail\":null}"}, "type": "function"}]}, {"role": "tool", "tool_call_id": "221154185", "content": "CallToolResult(content=[TextContent(type='text', text='[\"成都位置的天气还行\"]', annotations=None, meta=None)], structured_content={'result': ['成都位置的天气还行']}, meta=None, data=['成都位置的天气还行'], is_error=False)"}]
2025-12-13 16:27:38.759 DEBUG loop: loop.llm_request tools_payload=["get_current_weather_tool"]
2025-12-13 16:27:38.764 DEBUG openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': '你是一个会使用工具的助手。必要时调用工具，并在得到工具结果后给出最终回答。'}, {'role': 'user', 'content': '成都现在天气怎么样？'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'id': '221154185', 'function': {'name': 'get_current_weather_tool', 'arguments': '{"location":"成都","detail":null}'}, 'type': 'function'}]}, {'role': 'tool', 'tool_call_id': '221154185', 'content': 'CallToolResult(content=[TextContent(type=\'text\', text=\'["成都位置的天气还行"]\', annotations=None, meta=None)], structured_content={\'result\': [\'成都位置的天气还行\']}, meta=None, data=[\'成都位置的天气还行\'], is_error=False)'}], 'model': 'meta-llama-3.1-8b-instruct', 'parallel_tool_calls': False, 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather_tool', 'description': 'get weather of a location', 'parameters': {'properties': {'location': {'type': 'string'}, 'detail': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None}}, 'required': ['location'], 'type': 'object'}}}]}}
2025-12-13 16:27:38.766 DEBUG httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-12-13 16:27:38.766 DEBUG httpcore.http11: send_request_headers.complete
2025-12-13 16:27:38.766 DEBUG httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-12-13 16:27:38.766 DEBUG httpcore.http11: send_request_body.complete
2025-12-13 16:27:38.766 DEBUG httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-12-13 16:27:38.767 DEBUG httpcore.http11: receive_response_body.failed exception=GeneratorExit()
2025-12-13 16:27:40.320 DEBUG httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'564'), (b'ETag', b'W/"234-a8mQ0plGnomnTymKrInrF6XDs1A"'), (b'Date', b'Sat, 13 Dec 2025 15:27:40 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-13 16:27:40.321 INFO httpx: HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-13 16:27:40.321 DEBUG httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-12-13 16:27:40.321 DEBUG httpcore.http11: receive_response_body.complete
2025-12-13 16:27:40.321 DEBUG httpcore.http11: response_closed.started
2025-12-13 16:27:40.321 DEBUG httpcore.http11: response_closed.complete
2025-12-13 16:27:40.322 DEBUG openai._base_client: HTTP Request: POST http://localhost:1234/v1/chat/completions "200 OK"
2025-12-13 16:27:40.322 INFO loop: loop.llm_response message={"role": "assistant", "content": "根据工具结果，成都现在天气还行。", "tool_calls": []}
2025-12-13 16:27:40.322 DEBUG loop: loop.llm_response raw={"id": "chatcmpl-ielnum03n7brxya2fzuzm", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "根据工具结果，成都现在天气还行。", "refusal": null, "role": "assistant", "annotations": null, "audio": null, "function_call": null, "tool_calls": []}}], "created": 1765639658, "model": "meta-llama-3.1-8b-instruct", "object": "chat.completion", "service_tier": null, "system_fingerprint": "meta-llama-3.1-8b-instruct", "usage": {"completion_tokens": 13, "prompt_tokens": 483, "total_tokens": 496, "completion_tokens_details": null, "prompt_tokens_details": null}, "stats": {}}
2025-12-13 16:27:40.322 INFO loop: loop.turn start=4 current={"role": "assistant", "content": "根据工具结果，成都现在天气还行。", "tool_calls": []}
2025-12-13 16:27:40.322 INFO planner: planner.next_action input={"role": "assistant", "content": "根据工具结果，成都现在天气还行。", "tool_calls": []}
2025-12-13 16:27:40.323 INFO planner: planner.next_action output=Response len=16
2025-12-13 16:27:40.323 INFO loop: loop.action Response turn=4 len=16
2025-12-13 16:27:40.323 DEBUG loop: loop.final_response payload="根据工具结果，成都现在天气还行。"
2025-12-13 16:27:40.324 DEBUG httpcore.connection: connect_tcp.started host='0.0.0.0' port=8000 local_address=None timeout=30 socket_options=None
2025-12-13 16:27:40.325 DEBUG httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x106d556d0>
2025-12-13 16:27:40.325 DEBUG httpcore.http11: send_request_headers.started request=<Request [b'DELETE']>
2025-12-13 16:27:40.325 DEBUG httpcore.http11: send_request_headers.complete
2025-12-13 16:27:40.325 DEBUG httpcore.http11: send_request_body.started request=<Request [b'DELETE']>
2025-12-13 16:27:40.325 DEBUG httpcore.http11: send_request_body.complete
2025-12-13 16:27:40.325 DEBUG httpcore.http11: receive_response_headers.started request=<Request [b'DELETE']>
2025-12-13 16:27:40.326 DEBUG httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 13 Dec 2025 15:27:40 GMT'), (b'server', b'uvicorn'), (b'content-type', b'application/json'), (b'mcp-session-id', b'261d0aacc1814cb38056884a2d8a0d37'), (b'content-length', b'0')])
2025-12-13 16:27:40.326 INFO httpx: HTTP Request: DELETE http://0.0.0.0:8000/mcp "HTTP/1.1 200 OK"
2025-12-13 16:27:40.327 DEBUG httpcore.http11: receive_response_body.started request=<Request [b'DELETE']>
2025-12-13 16:27:40.327 DEBUG httpcore.http11: receive_response_body.complete
2025-12-13 16:27:40.327 DEBUG httpcore.http11: response_closed.started
2025-12-13 16:27:40.327 DEBUG httpcore.http11: response_closed.complete
2025-12-13 16:27:40.327 DEBUG httpcore.connection: close.started
2025-12-13 16:27:40.327 DEBUG httpcore.http11: receive_response_body.failed exception=CancelledError('Cancelled by cancel scope 107238510')
2025-12-13 16:27:40.327 DEBUG httpcore.http11: response_closed.started
2025-12-13 16:27:40.327 DEBUG httpcore.connection: close.complete
2025-12-13 16:27:40.327 DEBUG httpcore.connection: close.started
2025-12-13 16:27:40.327 DEBUG httpcore.http11: response_closed.complete
2025-12-13 16:27:40.328 DEBUG httpcore.connection: close.complete
